{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e4f5b4a-7d1c-443a-a60e-1186854abc6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------------+--------+-------------+\n|      Level|      Date and Time|      Source|Event ID|Task Category|\n+-----------+-------------------+------------+--------+-------------+\n|Information|19-12-2024 09:29:56| MSSQLSERVER|   17890|       Server|\n|Information|19-12-2024 09:28:32|  edgeupdate|       0|         None|\n|Information|19-12-2024 09:26:40|MsiInstaller|    1035|         None|\n|Information|19-12-2024 09:26:40|MsiInstaller|    1035|         None|\n|Information|19-12-2024 09:26:40|MsiInstaller|    1035|         None|\n+-----------+-------------------+------------+--------+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Set Azure Storage configurations\n",
    "storage_account_name = \"projectid3\"\n",
    "container_name = \"project\"\n",
    "storage_account_key = \"Sk4YdDVhS/tdri3VMFs09NGRrmuZTMFawnqXBq40hXi6qGzzh7cWLLxMLTf9WES0vlwWGVd3RCkg+AStmYEfQA==\"\n",
    "\n",
    "# Configure Spark to access Azure Blob Storage using the account key\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\",\n",
    "    storage_account_key\n",
    ")\n",
    "\n",
    "# Define the file path to the CSV file in Azure Blob Storage\n",
    "file_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/Application Logs.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "logs_df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "# Show the first few rows to confirm data is loaded correctly\n",
    "logs_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa4c7444-efd0-4a9a-8e25-1f4ee668fe77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------------+--------+-------------+\n|      Level|      Date and Time|      Source|Event ID|Task Category|\n+-----------+-------------------+------------+--------+-------------+\n|Information|19-12-2024 09:29:56| MSSQLSERVER|   17890|       Server|\n|Information|19-12-2024 09:28:32|  edgeupdate|       0|         None|\n|Information|19-12-2024 09:26:40|MsiInstaller|    1035|         None|\n|Information|19-12-2024 09:26:40|MsiInstaller|    1035|         None|\n|Information|19-12-2024 09:26:40|MsiInstaller|    1035|         None|\n+-----------+-------------------+------------+--------+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Loading the Delta Table\n",
    "# Load the Delta table from DBFS\n",
    "logs_df = spark.read.format(\"delta\").load(\"dbfs:/user/hive/warehouse/application_logs\")\n",
    "logs_df.createOrReplaceTempView(\"logs_view\")  # Create a temporary view for SQL queries\n",
    "\n",
    "# Display first 5 rows for a quick check\n",
    "logs_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d519d73-f606-47fb-ba76-e4afded41101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Level: string (nullable = true)\n |-- Date and Time: string (nullable = true)\n |-- Source: string (nullable = true)\n |-- Event ID: long (nullable = true)\n |-- Task Category: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Check the Schema of the DataFrame\n",
    "# This will help you understand the structure and available columns in your dataset.\n",
    "# Check the schema of the DataFrame to understand the available columns\n",
    "logs_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2d2b1fd-ebfe-4c4b-8c7b-c8e1e31488a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# 3. Calculating the Error Rate by Level\n",
    "# Assuming Level represents the severity of the log (e.g., \"error\", \"info\"), this step calculates the error rate.\n",
    "# Calculate error rate by 'Level' (assuming 'Level' is the severity column)\n",
    "error_count = logs_df.filter(logs_df[\"Level\"] == \"error\").count()\n",
    "total_count = logs_df.count()\n",
    "error_rate = (error_count / total_count) * 100\n",
    "\n",
    "print(f\"Error rate: {error_rate}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42963218-0c85-45d9-ab38-73eb2fe3f138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------------------+-----+\n|date|hour|               Level|count|\n+----+----+--------------------+-----+\n|NULL|NULL|C:\\WINDOWS\\system...|    7|\n|NULL|NULL|App: C:\\Program F...|    7|\n|NULL|NULL|\\t -s \"\"MSSQLSERV...|    7|\n|NULL|NULL|13: 8a292df8-d653...|    7|\n|NULL|NULL|Application Id=55...|    7|\n|NULL|NULL|dbv = 1568.230.50...|    1|\n|NULL|NULL|    10.0.22621.4601\"|    7|\n|NULL|NULL|[16] 0.001881 -0....|    1|\n|NULL|NULL|[1] 0.040380 -0.0...|    1|\n|NULL|NULL|               [5] -|    1|\n+----+----+--------------------+-----+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 4. Trend Analysis Over Time (Using Date and Time Column)\n",
    "# This step extracts the date and hour components from the Date and Time column to analyze the error trends over time.\n",
    "from pyspark.sql.functions import col, to_date, hour\n",
    "\n",
    "# Extract date and hour directly from 'Date and Time' column\n",
    "logs_df = logs_df.withColumn(\"date\", to_date(col(\"Date and Time\")))\n",
    "logs_df = logs_df.withColumn(\"hour\", hour(col(\"Date and Time\")))\n",
    "\n",
    "# Group by date and hour to find trends\n",
    "trend_df = logs_df.groupBy(\"date\", \"hour\", \"Level\").count().orderBy(\"date\", \"hour\")\n",
    "trend_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66819b46-9059-4fac-af90-16cd42bcc99e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No error logs found.\n+----+----+-----------+\n|date|hour|error_count|\n+----+----+-----------+\n+----+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Anomaly Detection Based on Error Count Spikes\n",
    "# Here, we detect anomalies by flagging times when the error count exceeds a predefined threshold (e.g., twice the average error count).\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Aggregating error counts by date and hour\n",
    "error_counts = logs_df.filter(logs_df.Level == \"error\") \\\n",
    "    .groupBy(\"date\", \"hour\") \\\n",
    "    .agg(F.count(\"*\").alias(\"error_count\"))\n",
    "\n",
    "# Calculate the average error count\n",
    "avg_error_count = error_counts.agg(F.avg(\"error_count\")).collect()[0][0]\n",
    "\n",
    "# Check if the average value is None (no errors in the logs), and set a default threshold if needed\n",
    "if avg_error_count is None:\n",
    "    print(\"No error logs found.\")\n",
    "    threshold = 10  # Set a reasonable default threshold if no errors are found\n",
    "else:\n",
    "    # Define a threshold for anomaly detection (e.g., twice the average error count)\n",
    "    threshold = avg_error_count * 2  # Double the average count\n",
    "\n",
    "# Flag anomalies where error count exceeds the threshold\n",
    "anomalies = error_counts.filter(error_counts.error_count > threshold)\n",
    "anomalies.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a9a66a-1467-4f72-99e3-86e6b519753e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------------+--------+-------------+----+----+---------+---------+-------+\n|Level      |Date and Time      |Source      |Event ID|Task Category|date|hour|timestamp|log_level|message|\n+-----------+-------------------+------------+--------+-------------+----+----+---------+---------+-------+\n|Information|19-12-2024 09:29:56|MSSQLSERVER |17890   |Server       |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:28:32|edgeupdate  |0       |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1033    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:40|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:39|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n|Information|19-12-2024 09:26:39|MsiInstaller|1035    |None         |NULL|NULL|         |         |       |\n+-----------+-------------------+------------+--------+-------------+----+----+---------+---------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Extract timestamp, log level, and message\n",
    "df_parsed = logs_df.withColumn(\"timestamp\", regexp_extract(\"Level\", r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\", 1)) \\\n",
    "              .withColumn(\"log_level\", regexp_extract(\"Level\", r\"\\b(INFO|ERROR|WARN|DEBUG)\\b\", 1)) \\\n",
    "              .withColumn(\"message\", regexp_extract(\"Level\", r\"(?:INFO|ERROR|WARN|DEBUG)\\s+(.*)\", 1))\n",
    "\n",
    "df_parsed.show(truncate=False)\n",
    "\n",
    "output_path = \"dbfs:/user/hive/warehouse/processed_logs\"\n",
    "\n",
    "# Save as Parquet\n",
    "df_parsed.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "# Save as CSV\n",
    "df_parsed.write.mode(\"overwrite\").csv(output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa67d95-9027-42d6-b2d3-864a070511e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to: wasbs://project@projectid3.blob.core.windows.net/processed_data/\n"
     ]
    }
   ],
   "source": [
    "# Define the output path for processed data\n",
    "output_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/processed_data/\"\n",
    "\n",
    "# Write the processed DataFrame back as a CSV file, overwrite existing data if necessary\n",
    "logs_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Data successfully written to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project ID-3 Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}